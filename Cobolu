#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
COBOL backtracker (no copybooks)

- Looks in ./cobol for .cbl/.cob/.txt
- Builds:
  * parent/level tree for variables (01/05/10/…)
  * MOVE dependency chains
  * READ <DD> INTO <buffer/record>
  * SELECT <DD> ASSIGN TO <literal>
- Writes variables.csv
- Prints backtrack for QUERY (e.g., "ALS-BOOKING-DATE")

This is heuristic and fast (regex-based), tuned to flows like:
ALS-BOOKING-DATE <- NMFMBKDT (MOVE)
NMFMBKDT ∈ RECORD-51-52-54
RECORD-51-52-54 <- WS-COMMON-AREA (MOVE)
WS-COMMON-AREA <- READ INPUT-FILE
SELECT INPUT-FILE ASSIGN TO IFILE
"""

import os, re, io, csv, glob
from collections import defaultdict, deque

# ===================== CONFIG =====================
COBOL_DIR   = os.path.join(os.getcwd(), "cobol")
OUTPUT_CSV  = os.path.join(os.getcwd(), "variables.csv")
# Set this to test a backtrack; leave "" to skip printing a trace.
QUERY       = "ALS-BOOKING-DATE"
# =================================================

# ----------- Helpers / regex ----------
IDENT = r"[A-Z0-9][A-Z0-9\-]*"
WS = r"[ \t]+"
DOT = r"\."

re_select_assign = re.compile(rf"\bSELECT{WS}+({IDENT}){WS}+ASSIGN{WS}+TO{WS}+([^\.\n]+){DOT}", re.I)
re_fd_or_sd      = re.compile(rf"^\s*(FD|SD){WS}+({IDENT})\s*{DOT}", re.I)
re_01            = re.compile(rf"^\s*01{WS}+({IDENT})\b", re.I)
re_level_item    = re.compile(rf"^\s*(\d{{2}}){WS}+({IDENT})\b", re.I)
re_read_into     = re.compile(rf"\bREAD{WS}+({IDENT})(?:{WS}+INTO{WS}+({IDENT}))?", re.I)
re_move          = re.compile(rf"\bMOVE{WS}+(.+?){WS}+TO{WS}+(.+?){DOT}", re.I)
re_compute       = re.compile(rf"\bCOMPUTE{WS}+({IDENT}){WS}*={WS}*(.+?){DOT}", re.I) # treat like MOVE from expr ids
re_division      = re.compile(rf"\b(PROCEDURE|ENVIRONMENT|DATA|IDENTIFICATION){WS}+DIVISION\b", re.I)

def norm(s: str) -> str:
    return re.sub(r"\s+", "", s.upper()) if isinstance(s, str) else s

def read_text(path: str) -> str:
    with io.open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

def is_comment_cobol(line: str) -> bool:
    if not line: return False
    if len(line) >= 7 and line[6] in ('*', '/'):  # fixed-format column 7
        return True
    if line.lstrip().startswith('*'):
        return True
    return False

def strip_inline_cobol(line: str) -> str:
    return re.sub(r"\*\>.*$", "", line)  # drop *> comments

def ids_in(text: str) -> set[str]:
    return set(norm(x) for x in re.findall(IDENT, text or "", flags=re.I))

# ----------- Data store (hash tables) -----------
class Store:
    def __init__(self):
        # variable -> meta
        self.vars = defaultdict(lambda: {
            "origin_file": None,          # where seen
            "defined_at": None,           # (file, line) for declaration or first sight
            "parent_record": None,        # nearest higher-level name (01/05/10…)
            "from_dd": set(),             # inferred DDs (via record/buffer binding)
            "assign_target": set(),       # SELECT DD ASSIGN TO ...
        })
        # relationships
        self.parent_of = {}               # child -> parent (single)
        self.children  = defaultdict(set) # parent -> {children}
        self.level     = {}               # var -> level number (as int), 1 for 01
        self.deps      = defaultdict(set) # target -> {sources} (from MOVE/COMPUTE)
        # IO bindings
        self.record_to_dd = {}            # record/buffer -> DD
        self.dd_assign    = {}            # DD -> ASSIGN literal

    def ensure_var(self, name: str, file: str, line: int):
        k = norm(name)
        v = self.vars[k]
        if v["origin_file"] is None:
            v["origin_file"] = file
        if v["defined_at"] is None:
            v["defined_at"] = (file, line)
        return k

    def set_parent(self, child: str, parent: str, level: int):
        c = norm(child); p = norm(parent)
        self.parent_of[c] = p
        self.children[p].add(c)
        self.level[c] = level
        self.vars[c]["parent_record"] = p

# ----------- COBOL scanner (no copybooks) -----------
def scan_cobol(path: str, store: Store):
    text = read_text(path)
    in_data_div = False
    in_fd = False
    current_01 = None
    current_dd = None

    # stack for levels: [(level_int, name)]
    stack = []

    for ln, raw in enumerate(text.splitlines(), 1):
        line = strip_inline_cobol(raw.rstrip("\n"))
        if not line.strip() or is_comment_cobol(line):
            continue

        # DIVISIONS
        m = re_division.search(line)
        if m:
            div = m.group(1).upper()
            in_data_div = (div == "DATA")
            # leaving FD tree when not in DATA
            in_fd = False if not in_data_div else in_fd
            continue

        # SELECT ... ASSIGN TO ...
        m = re_select_assign.search(line)
        if m:
            dd, assign = norm(m.group(1)), m.group(2).strip()
            store.dd_assign[dd] = assign
            continue

        # FD/SD start (binds following 01/levels to a DD)
        m = re_fd_or_sd.match(line)
        if m:
            in_fd = True
            current_dd = norm(m.group(2))
            current_01 = None
            stack = []
            continue

        if not in_data_div:
            # still allow MOVE/READ outside strict DATA detection
            pass

        # 01 level
        m = re_01.match(line)
        if m and in_fd:
            current_01 = norm(m.group(1))
            store.ensure_var(current_01, path, ln)
            store.level[current_01] = 1
            stack = [(1, current_01)]
            # bind this record to current DD if any
            if current_dd:
                store.record_to_dd.setdefault(current_01, current_dd)
            continue

        # other level items (02..49)
        m = re_level_item.match(line)
        if m and in_fd:
            lvl = int(m.group(1))
            name = norm(m.group(2))
            store.ensure_var(name, path, ln)
            # find parent on the stack (last with level < lvl)
            while stack and stack[-1][0] >= lvl:
                stack.pop()
            parent = stack[-1][1] if stack else current_01
            if parent:
                store.set_parent(name, parent, lvl)
            stack.append((lvl, name))
            continue

        # READ <DD> [INTO <buffer/record>]
        m = re_read_into.search(line)
        if m:
            dd = norm(m.group(1))
            into = m.group(2)
            if into:
                buf = norm(into)
                store.ensure_var(buf, path, ln)
                store.record_to_dd.setdefault(buf, dd)
            continue

        # MOVE src TO tgt1 tgt2 ...
        m = re_move.search(line)
        if m:
            src_expr = m.group(1).strip()
            srcs = ids_in(src_expr)
            tgts = [norm(t) for t in re.split(r"[,\s]+", m.group(2).strip()) if t]
            for t in tgts:
                store.ensure_var(t, path, ln)
                for s in srcs:
                    store.ensure_var(s, path, ln)
                    store.deps[t].add(s)
            continue

        # COMPUTE tgt = expr  (treat like MOVE from expr identifiers)
        m = re_compute.search(line)
        if m:
            tgt = norm(m.group(1))
            srcs = ids_in(m.group(2))
            store.ensure_var(tgt, path, ln)
            for s in srcs:
                store.ensure_var(s, path, ln)
                store.deps[tgt].add(s)
            continue

    # After scan: propagate DDs to children of any record/buffer already tied to a DD
    for rec, dd in list(store.record_to_dd.items()):
        # walk down its children
        q = deque([rec])
        seen = set([rec])
        while q:
            cur = q.popleft()
            store.vars[cur]["from_dd"].add(dd)
            for ch in store.children.get(cur, []):
                if ch not in seen:
                    seen.add(ch); q.append(ch)

    # Also: if a variable’s parent has a DD, inherit it
    for var, meta in store.vars.items():
        p = store.parent_of.get(var)
        if p:
            meta["parent_record"] = p
            for dd in store.vars[p]["from_dd"]:
                meta["from_dd"].add(dd)
        # attach assign target for any known DD
        for dd in list(meta["from_dd"]):
            at = store.dd_assign.get(dd)
            if at: meta["assign_target"].add(at)

# ----------- CSV writer -----------
def write_csv(store: Store, path: str):
    with io.open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow([
            "variable","origin_file","defined_at","parent_record",
            "from_dd","assign_target","direct_sources"
        ])
        for v, meta in sorted(store.vars.items()):
            loc = f"{meta['defined_at'][0]}:{meta['defined_at'][1]}" if meta["defined_at"] else ""
            w.writerow([
                v,
                meta.get("origin_file") or "",
                loc,
                meta.get("parent_record") or "",
                ";".join(sorted(meta.get("from_dd", []))) or "",
                ";".join(sorted(meta.get("assign_target", []))) or "",
                ";".join(sorted(store.deps.get(v, set()))) or "",
            ])

# ----------- Query: backtrack a variable to input file info -----------
def backtrack(store: Store, varname: str, max_depth: int = 20):
    """Return a human readable chain(s) ending at a DD/ASSIGN point."""
    start = norm(varname)
    if start not in store.vars:
        return [f"{varname}: not found"]

    out_lines = []
    # BFS with paths
    q = deque([[start]])
    seen = set([start])

    def end_at_dd(v) -> tuple[bool,str,str]:
        """True if v (or its parent chain) has a DD; returns (ok, dd, assign)."""
        cur = v
        hops = 0
        while cur and hops <= 8:
            if store.vars[cur]["from_dd"]:
                dd = sorted(store.vars[cur]["from_dd"])[0]
                at = store.dd_assign.get(dd, "")
                return True, dd, at
            cur = store.parent_of.get(cur)
            hops += 1
        return False, "", ""

    results = []
    while q:
        path = q.popleft()
        cur = path[-1]
        ok, dd, at = end_at_dd(cur)
        if ok:
            results.append((path, dd, at))
            continue
        if len(path) > max_depth:
            continue
        for src in sorted(store.deps.get(cur, set())):
            if (cur, src) not in seen:
                seen.add((cur, src))
                q.append(path + [src])

    if not results:
        return [f"{varname}: no DD/ASSIGN origin found via MOVE/COMPUTE chains."]

    # Pretty print each chain
    for path, dd, at in results:
        chain = "  <-  ".join(path)  # target to source flow
        line = f"{chain}    (DD={dd}; ASSIGN={at})"
        out_lines.append(line)
    return out_lines

# ----------- Driver -----------
def main():
    store = Store()
    # scan COBOL folder
    for path in glob.glob(os.path.join(COBOL_DIR, "**/*"), recursive=True):
        if os.path.isfile(path) and path.upper().endswith((".CBL",".COB",".TXT")):
            scan_cobol(path, store)

    write_csv(store, OUTPUT_CSV)
    print(f"Wrote {OUTPUT_CSV} with {len(store.vars)} variables.")

    if QUERY:
        print("\nBacktrack:")
        for line in backtrack(store, QUERY):
            print(" -", line)

if __name__ == "__main__":
    main()
