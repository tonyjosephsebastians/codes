#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
backtrace.py — ONLY copybook variables

Inputs (current folder):
  copybook/               (your copybooks: .cpy/.cob/.txt/.inc…)
  variables.csv           (from your COBOL parser)
  procs_index.csv         (from build_indexes.py)
  jcl_index.csv           (from build_indexes.py)
  sas_index.csv           (from build_indexes.py)

Output:
  enhanced_backtrace.csv with ONLY variables that appear in the copybooks.
"""

import os, io, re, csv, glob
from collections import defaultdict

BASE = os.getcwd()
CPY_DIR   = os.path.join(BASE, "copybook")
CSV_VARS  = os.path.join(BASE, "variables.csv")
CSV_PROCS = os.path.join(BASE, "procs_index.csv")
CSV_JCL   = os.path.join(BASE, "jcl_index.csv")
CSV_OUT   = os.path.join(BASE, "enhanced_backtrace.csv")

# Optional: set to a single copybook variable to print just its paths
QUERY = ""  # e.g., "ALS-BOOKING-DATE"

# ---------------- helpers ----------------
IDENT = r"[A-Z0-9][A-Z0-9\-]*"
def norm(s): return re.sub(r"\s+","",s.upper()) if isinstance(s,str) else s
def split_list(s): return [norm(x) for x in (s or "").split(";") if x.strip()]
def is_pseudo(n): return n.startswith("DD:") or n.startswith("ASSIGN:")

def read_text(p):
    with io.open(p, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

# -------------- build allow-list strictly from copybooks --------------
RE_ITEM = re.compile(rf"^\s*(\d{{2}})\s+({IDENT})\b", re.I)
RE_88   = re.compile(r"^\s*88\b", re.I)

def build_copybook_allowlist(copy_dir: str):
    """
    Return a set of *all* variable names defined in the copybooks (all levels),
    excluding 88-level condition names and FILLER.
    """
    allow = set()
    for path in glob.glob(os.path.join(copy_dir, "**/*"), recursive=True):
        if not os.path.isfile(path): 
            continue
        if not path.upper().endswith((".CPY",".CPB",".CBL",".COB",".TXT",".INC",".COPY",".CP")):
            continue

        for raw in read_text(path).splitlines():
            line = raw.rstrip("\n")
            if not line.strip() or RE_88.match(line):
                continue
            m = RE_ITEM.match(line)
            if not m:
                continue
            name = norm(m.group(2))
            if name != "FILLER":
                allow.add(name)
    return allow

# ---------------- variables.csv graph ----------------
def load_vars(csv_path):
    rows, parent, from_dd, assign, sources, origin_file = {}, {}, defaultdict(set), defaultdict(set), defaultdict(set), {}
    with io.open(csv_path, newline="", encoding="utf-8") as f:
        r = csv.DictReader(f)
        lower = {k.lower(): k for k in r.fieldnames}
        def get(row,key,default=""): return row.get(key, row.get(lower.get(key.lower(), ""), default))

        for row in r:
            v = norm(get(row,"variable"))
            if not v: continue
            rows[v] = row

            p = norm(get(row,"parent_record"))
            if p: parent[v] = p

            origin_file[v] = get(row,"origin_file","") or ""

            for dd in split_list(get(row,"from_dd","")):
                from_dd[v].add(dd)

            for at in split_list(get(row,"assign_target","")):
                assign[v].add(at)

            ds = get(row,"direct_sources") or get(row,"source_fields")
            for s in split_list(ds):
                sources[v].add(s)

    # keep only edges pointing to known vars
    known = set(rows.keys())
    for k in list(sources.keys()):
        sources[k] = {s for s in sources[k] if s in known}
    return rows, parent, from_dd, assign, sources, origin_file

def neighbors(idx, node):
    rows, parent, from_dd, assign, sources, _ = idx
    if node.startswith("ASSIGN:"):
        return set()
    if node.startswith("DD:"):
        dd = node[3:]
        hops=set()
        # DD -> any ASSIGN that co-exists on a var bound to that DD
        for v, ddset in from_dd.items():
            if dd in ddset:
                hops |= {f"ASSIGN:{at}" for at in assign.get(v,set())}
        return hops
    hops=set()
    p = parent.get(node)
    if p: hops.add(p)
    hops |= sources.get(node, set())
    for dd in from_dd.get(node,set()):
        hops.add(f"DD:{dd}")
    for at in assign.get(node,set()):
        hops.add(f"ASSIGN:{at}")
    return hops

def all_leaf_paths(idx, start_var, max_depth=2000):
    rows, *_ = idx
    start = norm(start_var)
    if start not in rows: return []
    res, stack, seen = [], [[start]], set()
    while stack:
        path = stack.pop()
        cur = path[-1]
        nxts = neighbors(idx, cur)
        if not nxts or len(path) > max_depth:
            res.append(path); continue
        for nxt in sorted(nxts):
            e=(cur,nxt)
            if e in seen: 
                continue
            seen.add(e)
            # avoid cycles on real variables (pseudo nodes are safe)
            if nxt in path and not is_pseudo(nxt):
                res.append(path); continue
            stack.append(path+[nxt])
    return res

# ---------------- load JCL / PROC / SAS indexes ----------------
def load_csv_rows(path):
    if not os.path.exists(path): return []
    with io.open(path, newline="", encoding="utf-8") as f:
        return list(csv.DictReader(f))

def build_lookups(procs_rows, jcl_rows, sas_rows):
    by_ddname_proc = defaultdict(list)
    by_ddname_jcl  = defaultdict(list)
    by_sashandle   = defaultdict(list)
    for r in procs_rows:
        by_ddname_proc[norm(r.get("ddname",""))].append(r)
    for r in jcl_rows:
        by_ddname_jcl[norm(r.get("ddname",""))].append(r)
    for r in sas_rows:
        h = r.get("handle_or_ds","")
        if h:
            by_sashandle[norm(h)].append(r)
    return by_ddname_proc, by_ddname_jcl, by_sashandle

def rows_to_str(rows, cols):
    if not rows: return ""
    return " || ".join("|".join(str(r.get(c,"")) for c in cols) for r in rows)

# ---------------- enhance a single path ----------------
def enhance_one_path(path, idx, lookups):
    by_ddname_proc, by_ddname_jcl, by_sashandle = lookups
    _, _, _, _, _, origin_file = idx

    copybook_var = path[0]

    assigns = [p[7:] for p in path if p.startswith("ASSIGN:")]
    dds     = [p[3:]  for p in path if p.startswith("DD:")]
    final_assign = assigns[-1] if assigns else ""
    final_dd     = dds[-1] if dds else ""

    key = norm(final_assign) if final_assign else norm(final_dd)
    proc_rows = by_ddname_proc.get(key, [])
    jcl_rows  = by_ddname_jcl.get(key, [])
    sas_rows  = by_sashandle.get(key, [])

    if final_assign and not (proc_rows or jcl_rows or sas_rows) and final_dd:
        key2 = norm(final_dd)
        proc_rows = by_ddname_proc.get(key2, [])
        jcl_rows  = by_ddname_jcl.get(key2, [])
        sas_rows  = by_sashandle.get(key2, [])

    file_hint = ""
    for n in path:
        if not is_pseudo(n):
            fh = origin_file.get(n,"")
            if fh: file_hint = fh; break

    return {
        "copybook_variable": copybook_var,
        "final_dd": final_dd,
        "final_assign": final_assign,
        "trace_path": " <- ".join(path),
        "jcl_rows": rows_to_str(jcl_rows,  ["file","step","exec","ddname","dsn","disp","recfm","lrecl"]),
        "proc_rows": rows_to_str(proc_rows, ["file","step","exec","ddname","dsn","disp","recfm","lrecl"]),
        "sas_rows": rows_to_str(sas_rows,   ["file","data_step","kind","handle_or_ds","raw"]),
        "cobol_file_hint": file_hint
    }

# ---------------- main ----------------
def main():
    # 1) Strict allow-list from copybooks
    os.makedirs(CPY_DIR, exist_ok=True)
    allow = build_copybook_allowlist(CPY_DIR)
    if not allow:
        print("Warning: copybook allow-list is empty (check ./copybook).")

    # 2) Load COBOL graph
    if not os.path.exists(CSV_VARS):
        print("variables.csv not found."); return
    idx = load_vars(CSV_VARS)

    # 3) Load indexes
    procs = load_csv_rows(CSV_PROCS)
    jcls  = load_csv_rows(CSV_JCL)
    sas   = load_csv_rows(CSV_SAS)
    lookups = build_lookups(procs, jcls, sas)

    # 4) Target ONLY variables that appear in the copybooks
    targets = sorted(v for v in idx[0].keys() if v in allow)
    if QUERY:
        targets = [norm(QUERY)] if norm(QUERY) in targets else []

    # 5) Build rows
    out=[]
    for var in targets:
        for path in all_leaf_paths(idx, var):
            out.append(enhance_one_path(path, idx, lookups))

    # 6) Write CSV
    with io.open(CSV_OUT, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(
            f,
            fieldnames=[
                "copybook_variable","final_dd","final_assign","trace_path",
                "jcl_rows","proc_rows","sas_rows","cobol_file_hint"
            ]
        )
        w.writeheader()
        for r in out: w.writerow(r)

    print(f"Wrote {CSV_OUT}  (rows={len(out)}). Scope: ONLY variables found in copybooks.")

if __name__ == "__main__":
    main()
