#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple COBOL variable search table generator.

- Loads COBOL program files from ./cobol/
- Loads COPY books from ./copybook/
- Builds a variables.csv table in the current directory
- Supports an inline query by setting the variable QUERY at the top

Usage:
    Place COBOL files in ./cobol and copybooks in ./copybook
    Run: python cobol_search_table.py
    It will produce variables.csv
    If QUERY is set (non-empty), it prints the traced input-file fields.
"""

import os, re, io, csv, glob
from collections import defaultdict, deque

# --------------- config
COBOL_DIR   = os.path.join(os.getcwd(), "cobol")
COPYBOOK_DIR= os.path.join(os.getcwd(), "copybook")
OUTPUT_CSV  = os.path.join(os.getcwd(), "variables.csv")
QUERY       = ""   # e.g. "CUSTOMER-NAME"

# --------------- helpers
IDENT = r"[A-Z0-9][A-Z0-9\-]*"
WS = r"[ \t]+"
DOT = r"\."

def norm(s: str) -> str:
    return re.sub(r"\s+", "", s.upper())

def read_text(path: str) -> str:
    with io.open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

def is_comment_cobol(line: str) -> bool:
    if not line: return False
    if len(line) >= 7 and line[6] in ('*', '/'):
        return True
    if line.lstrip().startswith('*'):
        return True
    return False

def strip_inline_cobol(line: str) -> str:
    return re.sub(r"\*\>.*$", "", line)

def ids_in(text: str) -> set[str]:
    return set(norm(x) for x in re.findall(IDENT, text or "", flags=re.I))

# --------------- regexes
re_copy_stmt      = re.compile(rf"^\s*COPY{WS}+({IDENT})(?:{WS}+REPLACING\b.*)?{DOT}", re.I)
re_select_assign  = re.compile(rf"\bSELECT{WS}+({IDENT}){WS}+ASSIGN{WS}+TO{WS}+([^\.\n]+){DOT}", re.I)
re_fd             = re.compile(rf"^\s*FD{WS}+({IDENT})\s*{DOT}", re.I)
re_01             = re.compile(rf"^\s*01{WS}+({IDENT})\b", re.I)
re_level_item     = re.compile(rf"^\s*(\d{{2}}){WS}+({IDENT})\b", re.I)
re_read_into      = re.compile(rf"\bREAD{WS}+({IDENT})(?:{WS}+INTO{WS}+({IDENT}))?", re.I)
re_move           = re.compile(rf"\bMOVE{WS}+(.+?){WS}+TO{WS}+(.+?){DOT}", re.I)
re_compute        = re.compile(rf"\bCOMPUTE{WS}+({IDENT}){WS}*={WS}*(.+?){DOT}", re.I)
re_call_using     = re.compile(rf"\bCALL{WS}+'([^']+)'{WS}+USING{WS}+(.+?){DOT}", re.I)

# --------------- store
class Store:
    def __init__(self):
        self.var_meta = defaultdict(lambda: {
            "parent_record": None,
            "source_fields": set(),
            "transitive_sources": set(),
            "related_subroutines": set(),
            "from_dd": set(),
            "assign_target": set(),
            "defined_at": None,
            "origin_file": None,
            "is_copybook": False,
        })
        self.record_to_dd = {}
        self.dd_assign = {}
        self.copy_member_to_records = defaultdict(set)
        self.copy_member_to_fields  = defaultdict(set)
        self.copy_member_files = {}
        self.fd_active_copy_members_by_file = defaultdict(set)
        self.deps = defaultdict(set)

    def merge_var(self, vname: str, **updates):
        key = norm(vname)
        v = self.var_meta[key]
        for k, val in updates.items():
            if k in ("source_fields","transitive_sources","related_subroutines","from_dd","assign_target"):
                if isinstance(val,(set,list,tuple)):
                    v[k].update(val)
                elif isinstance(val,str) and val:
                    v[k].add(val)
            elif k in ("defined_at","origin_file","is_copybook"):
                v[k] = val
            elif k=="parent_record":
                if not v["parent_record"] and val:
                    v["parent_record"]=norm(val)

# --------------- scanning
def scan_copybook(path, store: Store):
    member = norm(os.path.splitext(os.path.basename(path))[0])
    text = read_text(path)
    current_record=None
    for ln, raw in enumerate(text.splitlines(),1):
        line = strip_inline_cobol(raw)
        if not line.strip() or is_comment_cobol(line): continue
        m=re_01.match(line)
        if m: current_record=norm(m.group(1)); store.copy_member_to_records[member].add(current_record); continue
        m=re_level_item.match(line)
        if m and current_record:
            field=norm(m.group(2))
            store.copy_member_to_fields[member].add((current_record,field))
            store.merge_var(field,parent_record=current_record,defined_at=(path,ln),origin_file=path,is_copybook=True)
    store.copy_member_files[member]=path

def scan_cobol(path, store: Store):
    text=read_text(path)
    in_fd=False; current_dd=None; current_record=None
    for ln,raw in enumerate(text.splitlines(),1):
        line=strip_inline_cobol(raw)
        if not line.strip() or is_comment_cobol(line): continue
        m=re_select_assign.search(line)
        if m: dd,assign=norm(m.group(1)),m.group(2).strip(); store.dd_assign[dd]=assign; continue
        m=re_fd.match(line)
        if m: in_fd=True; current_dd=norm(m.group(1)); current_record=None; continue
        if in_fd:
            m=re_copy_stmt.match(line)
            if m:
                member=norm(m.group(1))
                store.fd_active_copy_members_by_file[path].add(member)
                for rec in store.copy_member_to_records.get(member,set()):
                    store.record_to_dd.setdefault(rec,current_dd)
                for rec,field in store.copy_member_to_fields.get(member,set()):
                    store.merge_var(field,parent_record=rec,origin_file=store.copy_member_files.get(member,path),is_copybook=True)
                    store.record_to_dd.setdefault(rec,current_dd)
                continue
        m=re_01.match(line)
        if m and in_fd:
            current_record=norm(m.group(1))
            if current_dd: store.record_to_dd.setdefault(current_record,current_dd)
            continue
        m=re_level_item.match(line)
        if m and in_fd and current_record:
            field=norm(m.group(2))
            store.merge_var(field,parent_record=current_record,defined_at=(path,ln),origin_file=path,is_copybook=False)
            continue
        m=re_read_into.search(line)
        if m:
            dd=norm(m.group(1)); into=m.group(2)
            if into: store.record_to_dd.setdefault(norm(into),dd)
            continue
        m=re_move.search(line)
        if m:
            src_ids=ids_in(m.group(1)); tgts=[norm(t) for t in re.split(r"[,\s]+",m.group(2).strip()) if t]
            for t in tgts: store.merge_var(t,defined_at=(path,ln),origin_file=path); store.var_meta[t]["source_fields"].update(src_ids); [store.deps[t].add(s) for s in src_ids]
            continue
        m=re_compute.search(line)
        if m:
            tgt=norm(m.group(1)); expr=m.group(2); src_ids=ids_in(expr)
            store.merge_var(tgt,defined_at=(path,ln),origin_file=path); store.var_meta[tgt]["source_fields"].update(src_ids); [store.deps[tgt].add(s) for s in src_ids]
            continue
        m=re_call_using.search(line)
        if m:
            sub=m.group(1); vars_=[norm(x) for x in re.findall(IDENT,m.group(2),flags=re.I)]
            for v in vars_: store.var_meta[v]["related_subroutines"].add(sub)

# --------------- resolve
def bind_dds(store: Store):
    for vname,meta in store.var_meta.items():
        parent=meta.get("parent_record")
        if parent:
            dd=store.record_to_dd.get(norm(parent))
            if dd:
                meta["from_dd"].add(dd)
                at=store.dd_assign.get(dd)
                if at: meta["assign_target"].add(at)

def propagate_sources(store: Store, max_depth=6):
    for target in list(store.var_meta.keys()):
        seen=set(); out=set(); q=deque([(target,0)])
        while q:
            cur,d=q.popleft()
            if d>=max_depth: continue
            for s in store.deps.get(cur,set()):
                if s not in seen:
                    seen.add(s); out.add(s); q.append((s,d+1))
        store.var_meta[target]["transitive_sources"].update(out)

# --------------- query
def find_input_sources(store: Store, varname: str, max_depth=10):
    start=norm(varname); seen={start}; q=deque([start]); results=[]
    while q:
        cur=q.popleft()
        meta=store.var_meta.get(cur)
        if not meta: continue
        parent=meta.get("parent_record")
        if parent:
            dd=store.record_to_dd.get(norm(parent))
            if dd:
                results.append((cur,parent,dd,",".join(meta.get("assign_target",[])),meta.get("origin_file")))
                continue
        for src in store.deps.get(cur,set())|meta.get("source_fields",set()):
            if src not in seen: seen.add(src); q.append(src)
    return results

# --------------- write CSV
def write_csv(store: Store, path: str):
    with io.open(path,"w",newline="",encoding="utf-8") as f:
        w=csv.writer(f)
        w.writerow(["variable","parent_record","source_fields","transitive_sources","related_subroutines","from_dd","assign_target","defined_at","origin_file","is_copybook"])
        for v,meta in sorted(store.var_meta.items()):
            loc=f"{meta['defined_at'][0]}:{meta['defined_at'][1]}" if meta['defined_at'] else ""
            w.writerow([v,meta.get("parent_record") or "",";".join(meta.get("source_fields",[])),";".join(meta.get("transitive_sources",[])),";".join(meta.get("related_subroutines",[])),";".join(meta.get("from_dd",[])),";".join(meta.get("assign_target",[])),loc,meta.get("origin_file") or "",meta.get("is_copybook")])

# --------------- main
def main():
    store=Store()
    # load copybooks
    for path in glob.glob(os.path.join(COPYBOOK_DIR,"**/*"),recursive=True):
        if os.path.isfile(path) and path.upper().endswith((".CPY",".CBL",".COB",".TXT")):
            scan_copybook(path,store)
    # load cobol
    for path in glob.glob(os.path.join(COBOL_DIR,"**/*"),recursive=True):
        if os.path.isfile(path) and path.upper().endswith((".CBL",".COB",".TXT")):
            scan_cobol(path,store)
    bind_dds(store)
    propagate_sources(store)
    write_csv(store,OUTPUT_CSV)
    print(f"variables.csv written with {len(store.var_meta)} variables")
    if QUERY:
        hits=find_input_sources(store,QUERY)
        if hits:
            print(f"Input sources for {QUERY}:")
            for h in hits:
                print(f" - {h[0]} (record={h[1]}, dd={h[2]}, assign={h[3]}, file={h[4]})")
        else:
            print(f"No input sources found for {QUERY}")

if __name__=="__main__":
    main()
